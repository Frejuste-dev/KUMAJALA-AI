"""
Script d'entra√Ænement du mod√®le de traduction
"""
import tensorflow as tf
from tensorflow import keras
import os
import argparse
from datetime import datetime

from ml.config import (
    EPOCHS, LEARNING_RATE, BATCH_SIZE, MODEL_DIR, LOGS_DIR, CHECKPOINTS_DIR,
    EARLY_STOPPING_PATIENCE, REDUCE_LR_PATIENCE, REDUCE_LR_FACTOR,
    SUPPORTED_LANGUAGES, TENSORBOARD_UPDATE_FREQ
)
from ml.data_preparation import DatasetBuilder
from ml.model_architecture import create_model
from ml.vocabulary import Vocabulary


class MaskedSparseCategoricalCrossentropy(keras.losses.Loss):
    """Loss function qui ignore le padding"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.loss_fn = keras.losses.SparseCategoricalCrossentropy(
            from_logits=True, reduction='none'
        )
    
    def call(self, y_true, y_pred):
        # Calculer la loss
        loss = self.loss_fn(y_true, y_pred)
        
        # Cr√©er un masque pour ignorer le padding (PAD_ID = 0)
        mask = tf.cast(tf.not_equal(y_true, 0), dtype=loss.dtype)
        
        # Appliquer le masque
        loss *= mask
        
        # Retourner la moyenne (en ignorant les valeurs masqu√©es)
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)


class TranslationTrainer:
    """Classe pour g√©rer l'entra√Ænement du mod√®le"""
    
    def __init__(self, target_language: str):
        self.target_language = target_language
        self.model = None
        self.source_vocab = None
        self.target_vocab = None
        self.history = None
        
        # Cr√©er un timestamp pour cette session d'entra√Ænement
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{target_language}_{self.timestamp}"
        
        # Chemins
        self.model_path = os.path.join(MODEL_DIR, f"{target_language}_model")
        self.checkpoint_path = os.path.join(CHECKPOINTS_DIR, self.run_name)
        self.log_dir = os.path.join(LOGS_DIR, self.run_name)
    
    def prepare_data(self, augment: bool = True):
        """Pr√©pare les donn√©es d'entra√Ænement"""
        print(f"\n{'='*60}")
        print(f"üìä PR√âPARATION DES DONN√âES")
        print(f"{'='*60}\n")
        
        builder = DatasetBuilder(self.target_language)
        train_ds, val_ds, test_ds = builder.prepare_all(augment=augment)
        
        # Sauvegarder les vocabulaires
        self.source_vocab = builder.source_vocab
        self.target_vocab = builder.target_vocab
        
        return train_ds, val_ds, test_ds
    
    def build_model(self):
        """Construit le mod√®le"""
        print(f"\n{'='*60}")
        print(f"üèóÔ∏è  CONSTRUCTION DU MOD√àLE")
        print(f"{'='*60}\n")
        
        if self.source_vocab is None or self.target_vocab is None:
            # Charger les vocabulaires si pas d√©j√† charg√©s
            self.source_vocab = Vocabulary.load(language='fr')
            self.target_vocab = Vocabulary.load(language=self.target_language)
        
        self.model = create_model(
            source_vocab_size=len(self.source_vocab),
            target_vocab_size=len(self.target_vocab)
        )
    
    def compile_model(self, learning_rate: float = LEARNING_RATE):
        """Compile le mod√®le"""
        print(f"\n‚öôÔ∏è  Compilation du mod√®le...")
        
        # Optimizer avec learning rate scheduling
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        
        # Loss function qui ignore le padding
        loss_fn = MaskedSparseCategoricalCrossentropy()
        
        # M√©triques
        metrics = [
            keras.metrics.SparseCategoricalAccuracy(name='accuracy')
        ]
        
        self.model.compile(
            optimizer=optimizer,
            loss=loss_fn,
            metrics=metrics,
            run_eagerly=True  # Force eager execution to avoid AutoGraph/Graph issues
        )
        
        print(f"‚úÖ Mod√®le compil√© (lr={learning_rate})")
    
    def get_callbacks(self):
        """Cr√©e les callbacks pour l'entra√Ænement"""
        callbacks = []
        
        # ModelCheckpoint - sauvegarder le meilleur mod√®le
        checkpoint_callback = keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(self.checkpoint_path, 'best_model.h5'),
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False,
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # EarlyStopping - arr√™ter si pas d'am√©lioration
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=EARLY_STOPPING_PATIENCE,
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)
        
        # ReduceLROnPlateau - r√©duire le learning rate
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=REDUCE_LR_FACTOR,
            patience=REDUCE_LR_PATIENCE,
            min_lr=1e-6,
            verbose=1
        )
        callbacks.append(reduce_lr)
        
        # TensorBoard - visualisation
        tensorboard = keras.callbacks.TensorBoard(
            log_dir=self.log_dir,
            update_freq=TENSORBOARD_UPDATE_FREQ,
            histogram_freq=1
        )
        callbacks.append(tensorboard)
        
        # CSV Logger - sauvegarder l'historique
        csv_logger = keras.callbacks.CSVLogger(
            os.path.join(self.log_dir, 'training_log.csv')
        )
        callbacks.append(csv_logger)
        
        return callbacks
    
    def train(self, train_ds, val_ds, epochs: int = EPOCHS):
        """Entra√Æne le mod√®le"""
        print(f"\n{'='*60}")
        print(f"üöÄ ENTRA√éNEMENT DU MOD√àLE")
        print(f"{'='*60}\n")
        print(f"Langue cible: {self.target_language}")
        print(f"Epochs: {epochs}")
        print(f"Logs: {self.log_dir}")
        print(f"Checkpoints: {self.checkpoint_path}\n")
        
        # Cr√©er les dossiers
        os.makedirs(self.checkpoint_path, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # Obtenir les callbacks
        callbacks = self.get_callbacks()
        
        # Entra√Æner
        self.history = self.model.fit(
            train_ds,
            validation_data=val_ds,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        
        print(f"\n‚úÖ Entra√Ænement termin√©!")
        
        return self.history
    
    def save_model(self):
        """Sauvegarde le mod√®le final"""
        print(f"\nüíæ Sauvegarde du mod√®le...")
        
        # Sauvegarder le mod√®le complet
        self.model.save(self.model_path)
        print(f"‚úÖ Mod√®le sauvegard√©: {self.model_path}")
        
        # Sauvegarder aussi les poids s√©par√©ment
        weights_path = os.path.join(MODEL_DIR, f"{self.target_language}_weights.h5")
        self.model.save_weights(weights_path)
        print(f"‚úÖ Poids sauvegard√©s: {weights_path}")
    
    def evaluate(self, test_ds):
        """√âvalue le mod√®le sur le test set"""
        print(f"\n{'='*60}")
        print(f"üìä √âVALUATION DU MOD√àLE")
        print(f"{'='*60}\n")
        
        results = self.model.evaluate(test_ds, verbose=1)
        
        print(f"\nüìà R√©sultats sur le test set:")
        print(f"   Loss: {results[0]:.4f}")
        print(f"   Accuracy: {results[1]:.4f}")
        
        return results
    
    def run_full_training(self, epochs: int = EPOCHS, augment: bool = True):
        """Pipeline complet d'entra√Ænement"""
        print(f"\n{'#'*70}")
        print(f"# ENTRA√éNEMENT COMPLET: FR ‚Üí {self.target_language.upper()}")
        print(f"{'#'*70}\n")
        
        # 1. Pr√©parer les donn√©es
        train_ds, val_ds, test_ds = self.prepare_data(augment=augment)
        
        # 2. Construire le mod√®le
        self.build_model()
        
        # 3. Compiler le mod√®le
        self.compile_model()
        
        # 4. Entra√Æner
        self.train(train_ds, val_ds, epochs=epochs)
        
        # 5. √âvaluer
        self.evaluate(test_ds)
        
        # 6. Sauvegarder
        self.save_model()
        
        print(f"\n{'#'*70}")
        print(f"# ‚úÖ ENTRA√éNEMENT TERMIN√â")
        print(f"{'#'*70}\n")
        
        print(f"üìÇ Fichiers g√©n√©r√©s:")
        print(f"   - Mod√®le: {self.model_path}")
        print(f"   - Logs: {self.log_dir}")
        print(f"   - Checkpoints: {self.checkpoint_path}")
        print(f"\nüí° Pour visualiser les logs:")
        print(f"   tensorboard --logdir {LOGS_DIR}")


def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description='Entra√Æner un mod√®le de traduction')
    parser.add_argument(
        '--target-language',
        type=str,
        choices=SUPPORTED_LANGUAGES,
        required=True,
        help='Langue cible pour la traduction'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=EPOCHS,
        help=f'Nombre d\'epochs (d√©faut: {EPOCHS})'
    )
    parser.add_argument(
        '--no-augment',
        action='store_true',
        help='D√©sactiver l\'augmentation de donn√©es'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=LEARNING_RATE,
        help=f'Learning rate (d√©faut: {LEARNING_RATE})'
    )
    
    args = parser.parse_args()
    
    # Cr√©er le trainer
    trainer = TranslationTrainer(args.target_language)
    
    # Lancer l'entra√Ænement complet
    trainer.run_full_training(
        epochs=args.epochs,
        augment=not args.no_augment
    )


if __name__ == "__main__":
    main()
